# Actory - 行動直結型ナレッジOS 最強要件定義書（実装フェーズ）

**バージョン**: 3.0.0  
**最終更新日**: 2024年12月21日  
**作成者**: Manus AI  
**プロジェクト名**: knowledge-os  
**ベースドキュメント**: requirements.md v2.0.0  
**技術検証レポート**: 技術的実現可能性およびアーキテクチャ検証レポート v2.0.0

---

## 目次

1. [エグゼクティブサマリー](#1-エグゼクティブサマリー)
2. [アーキテクチャ再設計](#2-アーキテクチャ再設計)
3. [実装優先順位とロードマップ](#3-実装優先順位とロードマップ)
4. [詳細機能要件（未実装機能）](#4-詳細機能要件未実装機能)
5. [技術的実装仕様](#5-技術的実装仕様)
6. [データベース拡張設計](#6-データベース拡張設計)
7. [API詳細設計](#7-api詳細設計)
8. [UI/UX詳細設計](#8-uiux詳細設計)
9. [パフォーマンス要件](#9-パフォーマンス要件)
10. [セキュリティ要件](#10-セキュリティ要件)
11. [テスト要件](#11-テスト要件)
12. [デプロイメント要件](#12-デプロイメント要件)

---

## 1. エグゼクティブサマリー

### 1.1 本ドキュメントの目的

本要件定義書は、`requirements.md`に記載された機能要件と、技術的実現可能性レポートの検証結果を統合し、**実装可能な形で詳細化**したものである。特に、以下の点を重視する：

- **技術的制約の明確化**: iOS PWAの限界、React 19の過渡期リスクなど、実装前に解決すべき技術的課題を明記
- **アーキテクチャの再設計**: 純粋なPWAではなく、Capacitorによるハイブリッドアプリ化の必要性
- **実装優先順位**: コア機能から段階的に実装するロードマップ
- **詳細な技術仕様**: 各機能の実装に必要な技術スタック、ライブラリ、設定を具体的に記載

### 1.2 重要なアーキテクチャ変更

| 項目 | 元の要件 | 修正後の要件 | 理由 |
|------|----------|--------------|------|
| モバイルアプリ | PWA（Web完結） | Capacitorハイブリッドアプリ | iOSバックグラウンド録音の制約 |
| フロントエンド | Next.js 15 + React 19 | Next.js 14 + React 18（初期） | エコシステムの安定性 |
| グラフ可視化 | D3.js (SVG) | Sigma.js (WebGL) | 大規模データのパフォーマンス |
| 音声分割 | クライアント側 | サーバー側FFmpeg | メモリ・バッテリー制約 |
| ベクトル検索 | 外部DB | Turso Native Vector Search | レイテンシ・同期の簡素化 |

### 1.3 実装フェーズ概要

**Phase 1 (Month 1): 基盤構築**
- Capacitor環境構築とiOS実機検証
- Next.js 14 + React 18環境での安定稼働確認
- Turso接続とベクトル検索の実装検証

**Phase 2 (Month 2): コア機能実装**
- 録音パイプライン（サーバーサイドFFmpeg + Whisper）
- Structured Outputsによるデータ構造化
- AI自動フォルダ管理の基本実装

**Phase 3 (Month 3): UI/UX実装**
- WebGL (Sigma.js) によるGraph View
- オンボーディング機能
- コンテキスト管理UI

**Phase 4 (Month 4+): 拡張機能**
- 外部サービス連携（Notion, Slack）
- コンテンツ生成機能
- Apple Watch録音機能

---

## 2. アーキテクチャ再設計

### 2.1 システム全体アーキテクチャ

```
┌─────────────────────────────────────────────────────────────┐
│                    Client Layer                              │
│  ┌───────────────────────────────────────────────────────┐  │
│  │  Web App (Next.js 14 + React 18)                     │  │
│  │  - PWA対応（デスクトップ・タブレット）                │  │
│  │  - WebGL (Sigma.js) によるGraph View                 │  │
│  └───────────────────────────────────────────────────────┘  │
│  ┌───────────────────────────────────────────────────────┐  │
│  │  Mobile App (Capacitor + Next.js)                    │  │
│  │  - iOS/Android ネイティブシェル                      │  │
│  │  - UIBackgroundModes によるバックグラウンド録音      │  │
│  │  - WatchConnectivity (Apple Watch)                   │  │
│  └───────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                  Server Layer (Next.js API Routes)          │
│  ┌───────────────────────────────────────────────────────┐  │
│  │  API Routes / Server Actions                         │  │
│  │  - tRPC Router (型安全なAPI)                         │  │
│  │  - BetterAuth (認証)                                 │  │
│  └───────────────────────────────────────────────────────┘  │
│  ┌───────────────────────────────────────────────────────┐  │
│  │  Background Workers                                   │  │
│  │  - FFmpeg音声処理ワーカー                            │  │
│  │  - Whisper API呼び出し                               │  │
│  │  - LLM処理（議事録生成、タスク抽出）                  │  │
│  └───────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                  Data Layer                                 │
│  ┌───────────────────────────────────────────────────────┐  │
│  │  Turso (LibSQL)                                       │  │
│  │  - DiskANN Vector Index                              │  │
│  │  - FTS5 Full-Text Search                             │  │
│  │  - Hybrid Search (Vector + FTS)                     │  │
│  └───────────────────────────────────────────────────────┘  │
│  ┌───────────────────────────────────────────────────────┐  │
│  │  S3 Compatible Storage                               │  │
│  │  - 音声ファイル保存                                  │  │
│  │  - 波形データ（Peaks.js用）                          │  │
│  └───────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                  External Services                          │
│  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐      │
│  │ OpenAI   │ │ Notion   │ │ Slack    │ │ Google   │      │
│  │ Whisper  │ │ API      │ │ API      │ │ OAuth    │      │
│  │ GPT-4    │ │          │ │          │ │          │      │
│  └──────────┘ └──────────┘ └──────────┘ └──────────┘      │
└─────────────────────────────────────────────────────────────┘
```

### 2.2 技術スタック詳細

#### 2.2.1 フロントエンド

| 技術 | バージョン | 用途 | 選定理由 |
|------|-----------|------|----------|
| Next.js | 14.2 (LTS) | フレームワーク | React 19の過渡期リスク回避 |
| React | 18.3 | UIライブラリ | エコシステムの安定性 |
| TypeScript | 5.3+ | 型安全性 | 型推論の強化 |
| Tailwind CSS | 4.0 | スタイリング | ユーティリティファースト |
| tRPC | 11.0+ | API通信 | 型安全なエンドツーエンド |
| Sigma.js | 2.4+ | グラフ可視化 | WebGLによる大規模描画 |
| Peaks.js | 3.4+ | 音声波形 | サーバーサイド生成データ表示 |

#### 2.2.2 モバイル

| 技術 | バージョン | 用途 | 選定理由 |
|------|-----------|------|----------|
| Capacitor | 6.0+ | ハイブリッドアプリ | UIBackgroundModesアクセス |
| @capacitor/audio-recorder | - | 録音プラグイン | ネイティブ録音API |
| @capacitor/filesystem | - | ファイルシステム | ローカルストレージ |
| @capacitor/background-runner | - | バックグラウンド処理 | 長時間タスク実行 |
| WatchConnectivity | - | Apple Watch連携 | watchOSアプリ通信 |

#### 2.2.3 バックエンド

| 技術 | バージョン | 用途 | 選定理由 |
|------|-----------|------|----------|
| Node.js | 20 LTS | ランタイム | 長期サポート |
| FFmpeg | 7.0+ | 音声処理 | 無音検知分割 |
| Drizzle ORM | 0.33+ | ORM | TypeScript型推論 |
| Turso | Latest | データベース | DiskANN Vector Search |

#### 2.2.4 AI/ML

| 技術 | バージョン | 用途 | 選定理由 |
|------|-----------|------|----------|
| OpenAI Whisper API | - | 文字起こし | 高精度日本語対応 |
| OpenAI GPT-4 | - | LLM処理 | Structured Outputs対応 |
| OpenAI Structured Outputs | - | データ構造化 | 100%型安全な出力 |

---

## 3. 実装優先順位とロードマップ

### 3.1 実装優先度マトリクス

| 機能 | ビジネス価値 | 技術的難易度 | 優先度 | フェーズ |
|------|------------|------------|--------|---------|
| オンボーディング機能 | 高 | 低 | P0 | Phase 1 |
| AI自動フォルダ管理 | 高 | 中 | P0 | Phase 2 |
| Capacitor環境構築 | 高 | 高 | P0 | Phase 1 |
| サーバーサイドFFmpeg | 高 | 中 | P0 | Phase 2 |
| Structured Outputs | 高 | 低 | P0 | Phase 2 |
| WebGL Graph View | 中 | 高 | P1 | Phase 3 |
| コンテキスト管理 | 高 | 高 | P1 | Phase 3 |
| 外部サービス連携 | 中 | 高 | P2 | Phase 4 |
| コンテンツ生成 | 中 | 中 | P2 | Phase 4 |
| Apple Watch録音 | 低 | 高 | P3 | Phase 5 |

### 3.2 詳細ロードマップ

#### Phase 1: 基盤構築（Month 1）

**Week 1-2: 環境構築と検証**
- [ ] Next.js 14 + React 18環境のセットアップ
- [ ] BetterAuthの動作確認（React 18環境）
- [ ] Turso接続とベクトル検索の実装検証
- [ ] Capacitor環境の構築（iOS/Android）
- [ ] iOS実機でのバックグラウンド録音検証

**Week 3-4: オンボーディング機能実装**
- [ ] オンボーディングUI実装（ステップバイステップ）
- [ ] 事業数・部数の入力フォーム
- [ ] フォルダ構造自動生成API
- [ ] 個人用/チーム用切り替え機能
- [ ] データベーススキーマ拡張（onboardingResponses）

#### Phase 2: コア機能実装（Month 2）

**Week 1-2: 録音パイプライン**
- [ ] サーバーサイドFFmpeg環境構築
- [ ] 無音検知分割アルゴリズム実装
- [ ] Whisper API統合（分割対応）
- [ ] 音声波形生成（Peaks.js用データ）
- [ ] 進捗表示UI

**Week 3-4: AI処理とデータ構造化**
- [ ] Structured Outputs実装
- [ ] 議事録生成プロンプト最適化
- [ ] タスク抽出（Function Calling）
- [ ] AI自動フォルダ分類の基本実装
- [ ] ユーザー修正の学習機能

#### Phase 3: UI/UX実装（Month 3）

**Week 1-2: Graph View実装**
- [ ] Sigma.js統合
- [ ] WebGLレンダリング設定
- [ ] ノード・エッジの可視化
- [ ] インタラクション（ズーム、パン、クリック）
- [ ] パフォーマンス最適化（仮想化）

**Week 3-4: コンテキスト管理UI**
- [ ] コンテキスト管理画面実装
- [ ] AI質問生成ロジック
- [ ] 毎日自動質問スケジューラー
- [ ] 階層構造ビュー実装
- [ ] プロジェクト詳細画面改善

#### Phase 4: 拡張機能（Month 4+）

**Week 1-2: 外部サービス連携**
- [ ] Notion API連携
- [ ] ページインポート機能
- [ ] AI再構成ロジック
- [ ] Slack API連携
- [ ] メッセージインポート

**Week 3-4: コンテンツ生成**
- [ ] 議事録直後提案機能
- [ ] 週次提案スケジューラー
- [ ] コンテンツ生成UI
- [ ] ユーザー口調学習
- [ ] テンプレート管理

---

## 4. 詳細機能要件（未実装機能）

### 4.1 AI自動フォルダ管理機能（詳細実装仕様）

#### 4.1.1 階層構造の自動生成アルゴリズム

**機能概要**: LLMを用いてコンテキストを分析し、適切な階層構造を自動生成する。

**詳細実装フロー**:

1. **コンテキスト分析フェーズ**
   ```
   入力: 録音テキスト or 議事録本文
   処理:
   - LLMに既存のフォルダ構造をコンテキストとして提供
   - 新しいコンテンツの内容を分析
   - 類似度の高い既存フォルダを検索（Turso Vector Search）
   - 階層構造の提案を生成
   ```

2. **Structured Outputsによる構造化**
   ```typescript
   const folderStructureSchema = {
     type: "object",
     properties: {
       suggestedPath: {
         type: "array",
         items: { type: "string" },
         description: "推奨されるフォルダパス（例: ['Aircle', 'オンラインイベント企画', '2025-12-27 Vimmerさんイベント']）"
       },
       confidence: {
         type: "number",
         minimum: 0,
         maximum: 100,
         description: "AIの信頼度スコア"
       },
       reasoning: {
         type: "string",
         description: "なぜこの階層構造を提案したかの理由"
       },
       alternativePaths: {
         type: "array",
         items: {
           type: "array",
           items: { type: "string" }
         },
         description: "代替案のフォルダパス"
       }
     },
     required: ["suggestedPath", "confidence"],
     additionalProperties: false
   };
   ```

3. **ユーザー修正の学習**
   - ユーザーが階層構造を修正した場合、修正内容を`folderCorrections`テーブルに保存
   - 次回の提案時に、過去の修正履歴をコンテキストとしてLLMに提供
   - 類似のコンテンツに対しては、学習した修正を自動適用

**技術実装**:

```typescript
// server/actions/ai-folder-classification.ts
import { openai } from "@/lib/openai";
import { db } from "@/db";
import { files, projects } from "@/db/schema";
import { eq } from "drizzle-orm";

export async function suggestFolderStructure(
  content: string,
  projectId: number,
  userId: number
) {
  // 1. 既存のフォルダ構造を取得
  const existingFolders = await db
    .select()
    .from(files)
    .where(eq(files.projectId, projectId));

  // 2. 類似コンテンツをベクトル検索
  const similarContent = await db.query.knowledge.findMany({
    where: (knowledge, { sql }) => sql`vector_search(${content}, 5)`,
  });

  // 3. LLMにStructured Outputsで提案を生成
  const completion = await openai.chat.completions.create({
    model: "gpt-4",
    messages: [
      {
        role: "system",
        content: `あなたはActoryのフォルダ構造管理AIです。
既存のフォルダ構造を参照し、新しいコンテンツを適切な階層に配置してください。
階層の深さは最大4段階までです。`,
      },
      {
        role: "user",
        content: `既存フォルダ構造:\n${JSON.stringify(existingFolders)}\n\n新しいコンテンツ:\n${content}\n\n類似コンテンツ:\n${JSON.stringify(similarContent)}`,
      },
    ],
    response_format: {
      type: "json_schema",
      json_schema: {
        name: "folder_structure_suggestion",
        schema: folderStructureSchema,
        strict: true,
      },
    },
  });

  return JSON.parse(completion.choices[0].message.content);
}
```

**データベース拡張**:

```typescript
// db/schema.ts に追加
export const folderCorrections = sqliteTable("folder_corrections", {
  id: integer("id").primaryKey(),
  userId: integer("user_id").notNull().references(() => users.id),
  projectId: integer("project_id").notNull().references(() => projects.id),
  originalPath: text("original_path").notNull(), // AIが提案したパス（JSON配列）
  correctedPath: text("corrected_path").notNull(), // ユーザーが修正したパス（JSON配列）
  content: text("content").notNull(), // 対象コンテンツ
  createdAt: integer("created_at", { mode: "timestamp" }).notNull(),
});
```

#### 4.1.2 録音時のプロジェクト選択UI

**UI仕様**:
- 録音開始前にモーダルを表示
- プロジェクト一覧を表示（最近使用した順）
- プロジェクトを選択すると、そのプロジェクトの階層構造をツリービューで表示
- 「スキップ」ボタンで後から分類可能
- 「AI提案」ボタンで自動分類を実行

**実装コンポーネント**:

```typescript
// components/recording/project-selector.tsx
"use client";

import { useState } from "react";
import { trpc } from "@/lib/trpc";
import { FolderTree } from "@/components/file/folder-tree";

export function ProjectSelector({
  onSelect,
  onSkip,
}: {
  onSelect: (projectId: number, folderPath: string[]) => void;
  onSkip: () => void;
}) {
  const { data: projects } = trpc.project.list.useQuery();
  const [selectedProjectId, setSelectedProjectId] = useState<number | null>(null);
  const { data: folderTree } = trpc.folder.getTree.useQuery(
    { projectId: selectedProjectId! },
    { enabled: !!selectedProjectId }
  );

  return (
    <div className="fixed inset-0 bg-black/50 flex items-center justify-center z-50">
      <div className="bg-card rounded-lg p-6 max-w-2xl w-full">
        <h2 className="text-2xl font-bold mb-4">プロジェクトを選択</h2>
        
        {/* プロジェクト一覧 */}
        <div className="grid grid-cols-2 gap-4 mb-6">
          {projects?.map((project) => (
            <button
              key={project.id}
              onClick={() => setSelectedProjectId(project.id)}
              className={`p-4 border rounded-lg ${
                selectedProjectId === project.id ? "border-primary" : ""
              }`}
            >
              {project.name}
            </button>
          ))}
        </div>

        {/* 階層構造表示 */}
        {selectedProjectId && folderTree && (
          <FolderTree
            tree={folderTree}
            onSelect={(path) => onSelect(selectedProjectId, path)}
          />
        )}

        <div className="flex gap-4 mt-6">
          <button onClick={onSkip} className="flex-1 btn-secondary">
            スキップ（後で分類）
          </button>
          {selectedProjectId && (
            <button
              onClick={() => {
                // AI提案を実行
              }}
              className="flex-1 btn-primary"
            >
              AI提案
            </button>
          )}
        </div>
      </div>
    </div>
  );
}
```

### 4.2 オンボーディング機能（詳細実装仕様）

#### 4.2.1 初期設定フローの詳細仕様

**ステップ1: プラン選択**
- 「Actory for one」または「Actory for company」を選択
- 選択に応じて次のステップが変わる

**ステップ2: 事業情報入力（Company版のみ）**
- 事業数の入力（1-10個）
- 各事業名の入力
- バリデーション: 事業名は必須、重複不可

**ステップ3: 部情報入力（Company版のみ）**
- 各事業ごとに部数を入力
- 各部の名前を入力
- 例: Aircle事業 → 3部 → 「営業部」「開発部」「マーケティング部」

**ステップ4: 主な用途の質問**
- 選択肢:
  - 会議記録の管理
  - タスク管理
  - ナレッジ蓄積
  - コンテンツ生成
  - その他（自由入力）
- 複数選択可能

**ステップ5: フォルダ構造の自動生成**
- 入力情報を基にLLMがフォルダ構造を生成
- プレビューを表示
- ユーザーが修正可能
- 「完了」ボタンで確定

**実装コード**:

```typescript
// app/(dashboard)/onboarding/page.tsx
"use client";

import { useState } from "react";
import { useRouter } from "next/navigation";
import { trpc } from "@/lib/trpc";

type PlanType = "one" | "company";
type Step = "plan" | "business" | "departments" | "purpose" | "preview";

export default function OnboardingPage() {
  const router = useRouter();
  const [step, setStep] = useState<Step>("plan");
  const [planType, setPlanType] = useState<PlanType | null>(null);
  const [businessCount, setBusinessCount] = useState(1);
  const [businessNames, setBusinessNames] = useState<string[]>([]);
  const [departmentCounts, setDepartmentCounts] = useState<Record<number, number>>({});
  const [departmentNames, setDepartmentNames] = useState<Record<number, string[]>>({});
  const [mainPurpose, setMainPurpose] = useState<string[]>([]);

  const generateStructure = trpc.onboarding.generateStructure.useMutation();
  const completeOnboarding = trpc.onboarding.complete.useMutation();

  const handleComplete = async () => {
    const folderStructure = await generateStructure.mutateAsync({
      planType: planType!,
      businessCount,
      businessNames,
      departmentCounts,
      departmentNames,
      mainPurpose,
    });

    await completeOnboarding.mutateAsync({
      planType: planType!,
      businessCount,
      businessNames,
      departmentCounts,
      departmentNames,
      mainPurpose,
      folderStructure,
    });

    router.push("/");
  };

  return (
    <div className="min-h-screen flex items-center justify-center bg-gradient-to-br from-background to-muted">
      <div className="max-w-2xl w-full p-8">
        {/* ステップインジケーター */}
        <div className="mb-8">
          <div className="flex items-center justify-between">
            {["plan", "business", "departments", "purpose", "preview"].map((s, i) => (
              <div
                key={s}
                className={`w-8 h-8 rounded-full flex items-center justify-center ${
                  step === s ? "bg-primary" : "bg-muted"
                }`}
              >
                {i + 1}
              </div>
            ))}
          </div>
        </div>

        {/* 各ステップのコンテンツ */}
        {step === "plan" && (
          <PlanSelectionStep
            onSelect={(plan) => {
              setPlanType(plan);
              setStep(plan === "one" ? "purpose" : "business");
            }}
          />
        )}

        {step === "business" && (
          <BusinessInputStep
            count={businessCount}
            names={businessNames}
            onCountChange={setBusinessCount}
            onNamesChange={setBusinessNames}
            onNext={() => setStep("departments")}
          />
        )}

        {/* ... 他のステップ */}
      </div>
    </div>
  );
}
```

#### 4.2.2 個人用/チーム用切り替え機能

**実装仕様**:
- ヘッダーまたはサイドバーに切り替えボタンを配置
- 切り替え時に確認ダイアログを表示
- 切り替え後、該当モードのデータのみを表示
- 個人用で作成した議事録を「チームに追加」ボタンで共有可能

**データベース拡張**:

```typescript
// filesテーブルに追加
export const files = sqliteTable("files", {
  // ... 既存カラム
  mode: text("mode", { enum: ["personal", "team"] }).default("personal"),
  sharedToTeam: integer("shared_to_team", { mode: "boolean" }).default(false),
});
```

### 4.3 組織共有機能（詳細実装仕様）

#### 4.3.1 Google OAuth連携と権限管理

**実装フロー**:

1. **Google OAuth設定**
   ```typescript
   // lib/auth/google-oauth.ts
   import { GoogleProvider } from "better-auth/providers";

   export const googleProvider = GoogleProvider({
     clientId: process.env.GOOGLE_CLIENT_ID!,
     clientSecret: process.env.GOOGLE_CLIENT_SECRET!,
     scope: ["email", "profile"],
   });
   ```

2. **権限設定UI**
   - フォルダごとに「共有」ボタンを表示
   - Googleアカウントのメールアドレスを入力
   - 権限レベルを選択（読み取り専用、編集可能、管理者）
   - 設定を保存

3. **権限チェックミドルウェア**
   ```typescript
   // server/middleware/permission-check.ts
   export async function checkFolderPermission(
     userId: number,
     fileId: number,
     requiredPermission: "read" | "write" | "admin"
   ): Promise<boolean> {
     const permission = await db.query.folderPermissions.findFirst({
       where: and(
         eq(folderPermissions.fileId, fileId),
         eq(folderPermissions.userId, userId)
       ),
     });

     if (!permission) return false;

     const permissionLevels = { read: 1, write: 2, admin: 3 };
     return (
       permissionLevels[permission.permission] >=
       permissionLevels[requiredPermission]
     );
   }
   ```

#### 4.3.2 自動アーカイブ機能

**実装仕様**:
- バッチジョブ（Cron）で1日1回実行
- `updatedAt`が1年以上前のファイルを検索
- `isArchived`フラグを`true`に設定
- アーカイブされたファイルは通常の検索から除外
- 「アーカイブを表示」トグルで表示可能

```typescript
// server/cron/archive-old-files.ts
import { db } from "@/db";
import { files } from "@/db/schema";
import { lt, eq } from "drizzle-orm";

export async function archiveOldFiles() {
  const oneYearAgo = new Date();
  oneYearAgo.setFullYear(oneYearAgo.getFullYear() - 1);
  const timestamp = Math.floor(oneYearAgo.getTime() / 1000);

  await db
    .update(files)
    .set({ isArchived: true })
    .where(
      and(
        lt(files.updatedAt, timestamp),
        eq(files.isArchived, false)
      )
    );
}
```

### 4.4 コンテキスト管理機能（詳細実装仕様）

#### 4.4.1 LLM-as-OS ファイルシステム抽象化

**概念**:
ナレッジを仮想的なファイルシステムとして管理し、AIエージェントが「ファイルを読み書き」する形でコンテキストにアクセスする。

**実装**:

```typescript
// lib/ai/file-system-abstraction.ts
export class VirtualFileSystem {
  constructor(private userId: number) {}

  async readFile(path: string): Promise<string> {
    // パスを解析してデータベースから取得
    // 例: /mnt/memories/2024/meeting_A.md
    const parts = path.split("/").filter(Boolean);
    // parts: ["mnt", "memories", "2024", "meeting_A.md"]
    
    if (parts[0] === "mnt" && parts[1] === "memories") {
      const year = parts[2];
      const fileName = parts[3];
      
      // データベースから該当する議事録を取得
      const note = await db.query.meetingNotes.findFirst({
        where: and(
          eq(meetingNotes.userId, this.userId),
          sql`strftime('%Y', datetime(${meetingNotes.createdAt}, 'unixepoch')) = ${year}`,
          like(meetingNotes.title, `%${fileName}%`)
        ),
      });

      return note?.formattedMinutes || "";
    }
    
    throw new Error(`File not found: ${path}`);
  }

  async writeFile(path: string, content: string): Promise<void> {
    // ファイルシステムへの書き込みをデータベース操作に変換
    // ...
  }

  async listDirectory(path: string): Promise<string[]> {
    // ディレクトリ内のファイル一覧を返す
    // ...
  }
}
```

**AIエージェントへのツール提供**:

```typescript
// lib/ai/context-agent.ts
import { VirtualFileSystem } from "./file-system-abstraction";

export function createContextAgentTools(userId: number) {
  const fs = new VirtualFileSystem(userId);

  return [
    {
      type: "function",
      function: {
        name: "read_file",
        description: "仮想ファイルシステムからファイルを読み込む",
        parameters: {
          type: "object",
          properties: {
            path: {
              type: "string",
              description: "ファイルパス（例: /mnt/memories/2024/meeting_A.md）",
            },
          },
          required: ["path"],
        },
      },
    },
    {
      type: "function",
      function: {
        name: "grep_files",
        description: "ファイル内を検索する",
        parameters: {
          type: "object",
          properties: {
            pattern: { type: "string" },
            directory: { type: "string" },
          },
          required: ["pattern"],
        },
      },
    },
    // ...
  ];
}
```

#### 4.4.2 毎日自動質問スケジューラー

**実装**:
- Edge FunctionsまたはVercel Cronで毎日実行
- その日に追加された議事録・文字起こしを分析
- LLMが質問を生成
- ユーザーに通知（プッシュ通知またはメール）

```typescript
// app/api/cron/daily-context-questions/route.ts
import { db } from "@/db";
import { meetingNotes, recordings } from "@/db/schema";
import { eq, gte } from "drizzle-orm";
import { openai } from "@/lib/openai";

export async function GET(request: Request) {
  // 認証チェック（Cron Secret）
  const authHeader = request.headers.get("authorization");
  if (authHeader !== `Bearer ${process.env.CRON_SECRET}`) {
    return new Response("Unauthorized", { status: 401 });
  }

  const today = new Date();
  today.setHours(0, 0, 0, 0);
  const timestamp = Math.floor(today.getTime() / 1000);

  // 今日追加されたデータを取得
  const newNotes = await db
    .select()
    .from(meetingNotes)
    .where(gte(meetingNotes.createdAt, timestamp));

  const newRecordings = await db
    .select()
    .from(recordings)
    .where(gte(recordings.createdAt, timestamp));

  // ユーザーごとにグループ化
  const userData = new Map<number, { notes: typeof newNotes; recordings: typeof newRecordings }>();
  
  for (const note of newNotes) {
    if (!userData.has(note.userId)) {
      userData.set(note.userId, { notes: [], recordings: [] });
    }
    userData.get(note.userId)!.notes.push(note);
  }

  // 各ユーザーに対して質問を生成
  for (const [userId, data] of userData) {
    const questions = await generateContextQuestions(userId, data);
    
    // コンテキスト管理セッションを作成
    await db.insert(contextManagementSessions).values({
      userId,
      targetFileIds: JSON.stringify([
        ...data.notes.map(n => n.fileId),
        ...data.recordings.map(r => r.fileId),
      ]),
      questions: JSON.stringify(questions),
      status: "active",
    });

    // ユーザーに通知
    await notifyUser(userId, questions);
  }

  return Response.json({ success: true });
}

async function generateContextQuestions(
  userId: number,
  data: { notes: typeof newNotes; recordings: typeof newRecordings }
): Promise<string[]> {
  const completion = await openai.chat.completions.create({
    model: "gpt-4",
    messages: [
      {
        role: "system",
        content: `あなたはActoryのコンテキスト管理AIです。
今日追加された議事録や録音データを分析し、ユーザーに質問をしてコンテキストを整理してください。
質問は3-5個程度に絞り、具体的で答えやすいものにしてください。`,
      },
      {
        role: "user",
        content: `今日追加されたデータ:\n${JSON.stringify(data, null, 2)}`,
      },
    ],
    response_format: {
      type: "json_schema",
      json_schema: {
        name: "context_questions",
        schema: {
          type: "object",
          properties: {
            questions: {
              type: "array",
              items: { type: "string" },
            },
          },
          required: ["questions"],
        },
        strict: true,
      },
    },
  });

  const result = JSON.parse(completion.choices[0].message.content);
  return result.questions;
}
```

### 4.5 外部サービス連携機能（詳細実装仕様）

#### 4.5.1 Notion連携

**実装フロー**:

1. **OAuth認証**
   ```typescript
   // lib/integrations/notion.ts
   import { Client } from "@notionhq/client";

   export async function connectNotion(accessToken: string) {
     const notion = new Client({ auth: accessToken });
     
     // ユーザー情報を取得
     const user = await notion.users.me();
     
     // データベースに保存
     await db.insert(externalIntegrations).values({
       userId: currentUserId,
       serviceType: "notion",
       accessToken: encrypt(accessToken), // 暗号化して保存
       serviceAccountId: user.id,
       isActive: true,
     });
   }
   ```

2. **ページインポート**
   ```typescript
   export async function importNotionPages(userId: number) {
     const integration = await db.query.externalIntegrations.findFirst({
       where: and(
        eq(externalIntegrations.userId, userId),
        eq(externalIntegrations.serviceType, "notion"),
        eq(externalIntegrations.isActive, true)
      ),
     });

     if (!integration) throw new Error("Notion not connected");

     const notion = new Client({ auth: decrypt(integration.accessToken) });
     
     // すべてのページを取得
     const pages = await notion.search({
       filter: { property: "object", value: "page" },
     });

     // 各ページをインポート
     for (const page of pages.results) {
       const pageContent = await notion.pages.retrieve({ page_id: page.id });
       const blocks = await notion.blocks.children.list({ block_id: page.id });
       
       // ブロックをMarkdownに変換
       const markdown = await convertNotionBlocksToMarkdown(blocks);
       
       // AIが再構成
       const reorganized = await reorganizeWithAI(markdown, userId);
       
       // データベースに保存
       await db.insert(importedContents).values({
         userId,
         integrationId: integration.id,
         contentType: "notion_page",
         originalId: page.id,
         title: extractTitle(pageContent),
         content: reorganized,
         metadata: JSON.stringify(pageContent),
       });
     }
   }
   ```

3. **AI再構成**
   ```typescript
   async function reorganizeWithAI(
     markdown: string,
     userId: number
   ): Promise<string> {
     // ユーザーの既存フォルダ構造を取得
     const folderStructure = await getFolderStructure(userId);
     
     const completion = await openai.chat.completions.create({
       model: "gpt-4",
       messages: [
         {
           role: "system",
           content: `あなたはNotionページをActoryのフォルダ構造に再構成するAIです。
ユーザーの既存フォルダ構造を参照し、適切に再構成してください。`,
         },
         {
           role: "user",
           content: `既存フォルダ構造:\n${JSON.stringify(folderStructure)}\n\nNotionページ:\n${markdown}`,
         },
       ],
     });

     return completion.choices[0].message.content;
   }
   ```

#### 4.5.2 Slack連携

**実装フロー**:

1. **OAuth認証（Slack App作成が必要）**
2. **メッセージ取得**
   - チャンネル一覧を取得
   - 各チャンネルのメッセージを取得
   - スレッドも含めて取得
3. **AI分類**
   - メッセージの内容を分析
   - 適切なプロジェクト・フォルダに分類
   - 口調情報も抽出して保存

### 4.6 コンテンツ生成機能（詳細実装仕様）

#### 4.6.1 議事録直後提案機能

**実装タイミング**:
- 議事録生成が完了した直後
- クライアント側で`meetingNote.generateMinutes`の完了を検知
- モーダルまたはトーストで提案を表示

**提案ロジック**:

```typescript
// server/actions/content-suggestion.ts
export async function suggestContentAfterMeetingNote(
  meetingNoteId: number
): Promise<ContentSuggestion[]> {
  const note = await db.query.meetingNotes.findFirst({
    where: eq(meetingNotes.id, meetingNoteId),
  });

  if (!note) throw new Error("Meeting note not found");

  // LLMが議事録の内容を分析して提案を生成
  const completion = await openai.chat.completions.create({
    model: "gpt-4",
    messages: [
      {
        role: "system",
        content: `あなたはActoryのコンテンツ生成提案AIです。
議事録の内容を分析し、以下の形式でコンテンツ生成を提案してください:
- 図解: チーム共有用の図解
- PDFマニュアル: 手順書やマニュアル
- Note記事: 経験談や学び
- X投稿: 簡潔なシェア
- YouTube台本: 動画コンテンツ`,
      },
      {
        role: "user",
        content: `議事録:\n${note.formattedMinutes}`,
      },
    ],
    response_format: {
      type: "json_schema",
      json_schema: {
        name: "content_suggestions",
        schema: {
          type: "object",
          properties: {
            suggestions: {
              type: "array",
              items: {
                type: "object",
                properties: {
                  contentType: {
                    type: "string",
                    enum: ["diagram", "pdf_manual", "note_article", "x_post", "youtube_script"],
                  },
                  title: { type: "string" },
                  reason: { type: "string" },
                  priority: { type: "string", enum: ["high", "medium", "low"] },
                },
                required: ["contentType", "title", "reason"],
              },
            },
          },
          required: ["suggestions"],
        },
        strict: true,
      },
    },
  });

  const result = JSON.parse(completion.choices[0].message.content);
  return result.suggestions;
}
```

#### 4.6.2 週次提案スケジューラー

**実装**:
- 毎週月曜日の朝9時に実行
- 過去1週間の議事録を分析
- 重要な決定事項や経験談を抽出
- コンテンツ生成を提案

```typescript
// app/api/cron/weekly-content-suggestions/route.ts
export async function GET(request: Request) {
  // 認証チェック
  const authHeader = request.headers.get("authorization");
  if (authHeader !== `Bearer ${process.env.CRON_SECRET}`) {
    return new Response("Unauthorized", { status: 401 });
  }

  const oneWeekAgo = new Date();
  oneWeekAgo.setDate(oneWeekAgo.getDate() - 7);
  const timestamp = Math.floor(oneWeekAgo.getTime() / 1000);

  // 過去1週間の議事録を取得
  const recentNotes = await db
    .select()
    .from(meetingNotes)
    .where(gte(meetingNotes.createdAt, timestamp));

  // ユーザーごとにグループ化
  const userNotes = new Map<number, typeof recentNotes>();
  for (const note of recentNotes) {
    if (!userNotes.has(note.userId)) {
      userNotes.set(note.userId, []);
    }
    userNotes.get(note.userId)!.push(note);
  }

  // 各ユーザーに対して提案を生成
  for (const [userId, notes] of userNotes) {
    const suggestions = await generateWeeklySuggestions(userId, notes);
    
    // ユーザーに通知
    await notifyUser(userId, {
      type: "weekly_content_suggestions",
      suggestions,
    });
  }

  return Response.json({ success: true });
}
```

### 4.7 口調管理機能（詳細実装仕様）

#### 4.7.1 口調学習アルゴリズム

**実装フロー**:

1. **メッセージ履歴の収集**
   - Slack、LINE、Discordからメッセージを取得
   - 送信者と受信者のペアを記録

2. **関係性の自動分類**
   ```typescript
   // lib/ai/tone-classification.ts
   export async function classifyRelationship(
     userId: number,
     contactId: string,
     messages: Message[]
   ): Promise<RelationshipType> {
     // ユーザーが送信したメッセージを分析
     const userMessages = messages.filter(m => m.senderId === userId);
     
     // LLMが関係性を判断
     const completion = await openai.chat.completions.create({
       model: "gpt-4",
       messages: [
         {
           role: "system",
           content: `あなたは口調分析AIです。メッセージの口調から、相手との関係性を4段階で分類してください:
1. superior: 最上級の目上の人（非常に丁寧、敬語）
2. boss: 上司（丁寧だが親しみも）
3. peer: 仲間、友達（カジュアル）
4. subordinate: 部下（やや上から目線）`,
         },
         {
           role: "user",
           content: `メッセージ履歴:\n${JSON.stringify(userMessages)}`,
         },
       ],
       response_format: {
         type: "json_schema",
         json_schema: {
           name: "relationship_classification",
           schema: {
             type: "object",
             properties: {
               relationshipType: {
                 type: "string",
                 enum: ["superior", "boss", "peer", "subordinate"],
               },
               confidence: { type: "number" },
               examples: {
                 type: "array",
                 items: { type: "string" },
               },
             },
             required: ["relationshipType", "confidence"],
           },
           strict: true,
         },
       },
     });

     const result = JSON.parse(completion.choices[0].message.content);
     return result.relationshipType;
   }
   ```

3. **口調パターンの抽出**
   - メッセージから特徴的な表現を抽出
   - 「です・ます調」「だ・である調」「ですよね」などのパターンを記録

4. **データベースへの保存**
   ```typescript
   await db.insert(relationshipProfiles).values({
     userId,
     contactName: contact.name,
     contactEmail: contact.email,
     contactSlackId: contact.slackId,
     relationshipType: classifiedType,
     tone: JSON.stringify({
       patterns: extractedPatterns,
       examples: exampleMessages,
     }),
     context: JSON.stringify({
       projectContext: projectContext,
       frequency: messageFrequency,
     }),
   });
   ```

#### 4.7.2 返信生成時の口調適用

**実装**:

```typescript
// lib/ai/tone-aware-reply.ts
export async function generateToneAwareReply(
  userId: number,
  contactId: string,
  message: string,
  context?: string
): Promise<string> {
  // 関係性プロフィールを取得
  const profile = await db.query.relationshipProfiles.findFirst({
    where: and(
      eq(relationshipProfiles.userId, userId),
      or(
        eq(relationshipProfiles.contactEmail, contactId),
        eq(relationshipProfiles.contactSlackId, contactId)
      )
    ),
  });

  if (!profile) {
    // プロフィールがない場合はデフォルトの口調で生成
    return await generateDefaultReply(userId, message, context);
  }

  const toneInfo = JSON.parse(profile.tone);

  // LLMに口調情報をコンテキストとして提供
  const completion = await openai.chat.completions.create({
    model: "gpt-4",
    messages: [
      {
        role: "system",
        content: `あなたはActoryの返信生成AIです。
ユーザーの過去のメッセージから学習した口調パターンに従って、適切な返信を生成してください。

関係性: ${profile.relationshipType}
口調パターン: ${JSON.stringify(toneInfo.patterns)}
例: ${JSON.stringify(toneInfo.examples)}`,
      },
      {
        role: "user",
        content: `元のメッセージ: ${message}\n\nコンテキスト: ${context || "なし"}`,
      },
    ],
  });

  return completion.choices[0].message.content;
}
```

---

## 5. 技術的実装仕様

### 5.1 Capacitor環境構築

#### 5.1.1 セットアップ手順

```bash
# 1. Capacitorをインストール
npm install @capacitor/core @capacitor/cli
npm install @capacitor/ios @capacitor/android

# 2. Capacitorを初期化
npx cap init

# 3. iOSプラットフォームを追加
npx cap add ios

# 4. 必要なプラグインをインストール
npm install @capacitor/audio-recorder
npm install @capacitor/filesystem
npm install @capacitor/background-runner
```

#### 5.1.2 iOS設定

**Info.plist設定**:

```xml
<key>UIBackgroundModes</key>
<array>
  <string>audio</string>
  <string>fetch</string>
  <string>processing</string>
</array>

<key>NSMicrophoneUsageDescription</key>
<string>Actoryは会議や打ち合わせを録音するためにマイクへのアクセスが必要です。</string>

<key>NSPhotoLibraryUsageDescription</key>
<string>Actoryは音声ファイルを保存するためにフォトライブラリへのアクセスが必要です。</string>
```

**Capacitor設定**:

```typescript
// capacitor.config.ts
import { CapacitorConfig } from '@capacitor/cli';

const config: CapacitorConfig = {
  appId: 'com.actory.app',
  appName: 'Actory',
  webDir: 'out',
  server: {
    androidScheme: 'https',
    iosScheme: 'https',
  },
  plugins: {
    AudioRecorder: {
      enableBackgroundRecording: true,
    },
    BackgroundRunner: {
      enabled: true,
    },
  },
};

export default config;
```

#### 5.1.3 バックグラウンド録音の実装

```typescript
// lib/capacitor/audio-recorder.ts
import { AudioRecorder } from '@capacitor/audio-recorder';
import { Filesystem } from '@capacitor/filesystem';

export class CapacitorAudioRecorder {
  private recording = false;
  private startTime: number | null = null;

  async startRecording(): Promise<void> {
    try {
      // マイクの権限を確認
      const { status } = await AudioRecorder.checkPermissions();
      if (status !== 'granted') {
        const { status: newStatus } = await AudioRecorder.requestPermissions();
        if (newStatus !== 'granted') {
          throw new Error('Microphone permission denied');
        }
      }

      // 録音設定
      await AudioRecorder.start({
        source: 'microphone',
        quality: 'high',
        format: 'mp4',
        channels: 1,
        sampleRate: 48000,
      });

      this.recording = true;
      this.startTime = Date.now();
    } catch (error) {
      console.error('Failed to start recording:', error);
      throw error;
    }
  }

  async stopRecording(): Promise<{ filePath: string; duration: number }> {
    if (!this.recording) {
      throw new Error('Not recording');
    }

    try {
      const result = await AudioRecorder.stop();
      const duration = this.startTime ? Date.now() - this.startTime : 0;

      // ファイルをサーバーにアップロード
      const fileData = await Filesystem.readFile({
        path: result.filePath,
      });

      const uploadResult = await uploadToServer(
        fileData.data,
        result.filePath,
        duration
      );

      this.recording = false;
      this.startTime = null;

      return {
        filePath: uploadResult.url,
        duration: Math.floor(duration / 1000),
      };
    } catch (error) {
      console.error('Failed to stop recording:', error);
      throw error;
    }
  }

  async uploadToServer(
    fileData: string,
    fileName: string,
    duration: number
  ): Promise<{ url: string }> {
    // Base64データをBlobに変換
    const blob = await fetch(`data:audio/mp4;base64,${fileData}`).then(
      (r) => r.blob()
    );

    // サーバーにアップロード
    const formData = new FormData();
    formData.append('audio', blob, fileName);
    formData.append('duration', duration.toString());

    const response = await fetch('/api/recordings/upload', {
      method: 'POST',
      body: formData,
    });

    if (!response.ok) {
      throw new Error('Upload failed');
    }

    const result = await response.json();
    return { url: result.url };
  }
}
```

### 5.2 サーバーサイドFFmpeg実装

#### 5.2.1 FFmpeg環境構築

**Dockerfile例**:

```dockerfile
FROM node:20-slim

# FFmpegをインストール
RUN apt-get update && apt-get install -y ffmpeg && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY package*.json ./
RUN npm ci

COPY . .

CMD ["npm", "run", "start"]
```

#### 5.2.2 無音検知分割の実装

```typescript
// server/workers/audio-splitter.ts
import { exec } from 'child_process';
import { promisify } from 'util';
import { writeFile, readFile } from 'fs/promises';
import { tmpdir } from 'os';
import { join } from 'path';

const execAsync = promisify(exec);

export interface AudioSplitResult {
  chunks: Array<{
    path: string;
    startTime: number;
    endTime: number;
    duration: number;
  }>;
  totalDuration: number;
}

export async function splitAudioBySilence(
  inputPath: string,
  maxChunkSizeMB: number = 25
): Promise<AudioSplitResult> {
  const outputDir = join(tmpdir(), `audio-split-${Date.now()}`);
  await execAsync(`mkdir -p ${outputDir}`);

  // 1. 無音検知ポイントを検出
  const silenceDetectCmd = `ffmpeg -i ${inputPath} -af silencedetect=n=-30dB:d=0.5 -f null - 2>&1`;
  const { stdout } = await execAsync(silenceDetectCmd);

  // 2. 無音ポイントをパース
  const silencePoints = parseSilenceDetectOutput(stdout);
  
  // 3. 25MBを超えない範囲でチャンクを分割
  const chunks = await createChunks(
    inputPath,
    silencePoints,
    outputDir,
    maxChunkSizeMB
  );

  return {
    chunks,
    totalDuration: chunks.reduce((sum, chunk) => sum + chunk.duration, 0),
  };
}

function parseSilenceDetectOutput(output: string): number[] {
  const silencePoints: number[] = [0]; // 開始点
  
  const silenceStartRegex = /silence_start: ([\d.]+)/g;
  const silenceEndRegex = /silence_end: ([\d.]+)/g;
  
  let match;
  while ((match = silenceStartRegex.exec(output)) !== null) {
    silencePoints.push(parseFloat(match[1]));
  }
  
  return silencePoints.sort((a, b) => a - b);
}

async function createChunks(
  inputPath: string,
  silencePoints: number[],
  outputDir: string,
  maxChunkSizeMB: number
): Promise<AudioSplitResult['chunks']> {
  const chunks: AudioSplitResult['chunks'] = [];
  let currentChunkStart = 0;
  let chunkIndex = 0;

  for (let i = 1; i < silencePoints.length; i++) {
    const potentialEnd = silencePoints[i];
    const duration = potentialEnd - currentChunkStart;
    
    // ファイルサイズを推定（MP3 128kbpsの場合）
    const estimatedSizeMB = (duration * 128) / (8 * 1024 * 1024);

    if (estimatedSizeMB > maxChunkSizeMB) {
      // 現在のチャンクを保存
      const chunkPath = join(outputDir, `chunk-${chunkIndex}.mp3`);
      await execAsync(
        `ffmpeg -i ${inputPath} -ss ${currentChunkStart} -t ${duration} -acodec libmp3lame -b:a 128k ${chunkPath}`
      );

      chunks.push({
        path: chunkPath,
        startTime: currentChunkStart,
        endTime: potentialEnd,
        duration,
      });

      currentChunkStart = potentialEnd;
      chunkIndex++;
    }
  }

  // 最後のチャンク
  if (currentChunkStart < silencePoints[silencePoints.length - 1]) {
    const chunkPath = join(outputDir, `chunk-${chunkIndex}.mp3`);
    const finalDuration = silencePoints[silencePoints.length - 1] - currentChunkStart;
    
    await execAsync(
      `ffmpeg -i ${inputPath} -ss ${currentChunkStart} -t ${finalDuration} -acodec libmp3lame -b:a 128k ${chunkPath}`
    );

    chunks.push({
      path: chunkPath,
      startTime: currentChunkStart,
      endTime: silencePoints[silencePoints.length - 1],
      duration: finalDuration,
    });
  }

  return chunks;
}
```

#### 5.2.3 Whisper API統合（分割対応）

```typescript
// server/actions/transcribe-split-audio.ts
import { splitAudioBySilence } from '../workers/audio-splitter';
import { openai } from '@/lib/openai';
import { readFile } from 'fs/promises';

export async function transcribeLargeAudio(
  audioPath: string,
  audioKey: string
): Promise<string> {
  // 1. 音声ファイルを分割
  const splitResult = await splitAudioBySilence(audioPath);

  // 2. 各チャンクをWhisper APIで文字起こし
  const transcriptions: string[] = [];
  
  for (const chunk of splitResult.chunks) {
    const audioFile = await readFile(chunk.path);
    const file = new File([audioFile], `chunk-${chunk.startTime}.mp3`, {
      type: 'audio/mpeg',
    });

    const transcription = await openai.audio.transcriptions.create({
      file: file,
      model: 'whisper-1',
      language: 'ja',
      response_format: 'verbose_json',
      prompt: '日本語の会議の文字起こしです。句読点を適切に挿入してください。',
    });

    transcriptions.push(transcription.text);
  }

  // 3. 文字起こし結果を統合
  // 重複を除去し、時系列で結合
  const mergedTranscription = mergeTranscriptions(transcriptions, splitResult.chunks);

  return mergedTranscription;
}

function mergeTranscriptions(
  transcriptions: string[],
  chunks: AudioSplitResult['chunks']
): string {
  // 重複する部分を検出して除去
  // 簡易実装: 最後の文と次の最初の文が重複している可能性を考慮
  let merged = transcriptions[0];
  
  for (let i = 1; i < transcriptions.length; i++) {
    const prev = transcriptions[i - 1];
    const current = transcriptions[i];
    
    // 重複を検出（簡易版）
    const overlap = findOverlap(prev, current);
    if (overlap.length > 0) {
      merged += current.substring(overlap.length);
    } else {
      merged += ' ' + current;
    }
  }

  return merged;
}

function findOverlap(str1: string, str2: string): string {
  // 最後のN文字と最初のN文字が一致するかチェック
  const maxOverlap = Math.min(str1.length, str2.length, 100);
  
  for (let i = maxOverlap; i > 0; i--) {
    const suffix = str1.substring(str1.length - i);
    const prefix = str2.substring(0, i);
    
    if (suffix === prefix) {
      return suffix;
    }
  }
  
  return '';
}
```

### 5.3 WebGL Graph View実装

#### 5.3.1 Sigma.js統合

```typescript
// components/graph/sigma-graph-view.tsx
"use client";

import { useEffect, useRef } from 'react';
import Sigma from 'sigma';
import { Graph } from 'graphology';
import { trpc } from '@/lib/trpc';

interface SigmaGraphViewProps {
  projectId: number;
}

export function SigmaGraphView({ projectId }: SigmaGraphViewProps) {
  const containerRef = useRef<HTMLDivElement>(null);
  const sigmaRef = useRef<Sigma | null>(null);
  
  const { data: graphData } = trpc.link.getGraph.useQuery({ projectId });

  useEffect(() => {
    if (!containerRef.current || !graphData) return;

    // Graphologyグラフを作成
    const graph = new Graph();
    
    // ノードを追加
    graphData.nodes.forEach((node) => {
      graph.addNode(node.id, {
        label: node.label,
        size: node.size || 10,
        color: node.color || '#00D4AA',
        x: Math.random() * 100,
        y: Math.random() * 100,
      });
    });

    // エッジを追加
    graphData.edges.forEach((edge) => {
      graph.addEdge(edge.source, edge.target, {
        label: edge.label,
        size: edge.size || 1,
        color: edge.color || '#666',
      });
    });

    // Sigma.jsインスタンスを作成
    const sigma = new Sigma(graph, containerRef.current, {
      renderer: {
        type: 'webgl',
      },
      settings: {
        defaultNodeColor: '#00D4AA',
        defaultEdgeColor: '#666',
        labelFont: 'Inter, sans-serif',
        labelSize: 12,
        labelWeight: 'normal',
        zIndex: true,
      },
    });

    sigmaRef.current = sigma;

    // クリーンアップ
    return () => {
      sigma.kill();
    };
  }, [graphData]);

  return (
    <div className="w-full h-screen">
      <div ref={containerRef} className="w-full h-full" />
    </div>
  );
}
```

#### 5.3.2 パフォーマンス最適化

**仮想化（Virtualization）**:
- 画面に表示されている範囲のノードのみをレンダリング
- ズームレベルに応じてノードの詳細度を調整

```typescript
// lib/graph/virtualization.ts
export function getVisibleNodes(
  camera: { x: number; y: number; ratio: number },
  graph: Graph,
  viewport: { width: number; height: number }
): string[] {
  const visibleNodes: string[] = [];
  const bounds = {
    left: camera.x - viewport.width / (2 * camera.ratio),
    right: camera.x + viewport.width / (2 * camera.ratio),
    top: camera.y - viewport.height / (2 * camera.ratio),
    bottom: camera.y + viewport.height / (2 * camera.ratio),
  };

  graph.forEachNode((nodeId, attributes) => {
    if (
      attributes.x >= bounds.left &&
      attributes.x <= bounds.right &&
      attributes.y >= bounds.top &&
      attributes.y <= bounds.bottom
    ) {
      visibleNodes.push(nodeId);
    }
  });

  return visibleNodes;
}
```

### 5.4 Turso Vector Search実装

#### 5.4.1 DiskANNインデックスの作成

```typescript
// server/actions/vector-search.ts
import { db } from '@/db';
import { sql } from 'drizzle-orm';

export async function createVectorIndex() {
  // DiskANNインデックスを作成
  await db.execute(sql`
    CREATE VIRTUAL TABLE knowledge_vector_index USING diskann(
      id INTEGER PRIMARY KEY,
      embedding VECTOR(1536),
      content TEXT
    );
  `);
}

export async function insertVectorEmbedding(
  knowledgeId: number,
  embedding: number[],
  content: string
) {
  await db.execute(sql`
    INSERT INTO knowledge_vector_index (id, embedding, content)
    VALUES (${knowledgeId}, ${JSON.stringify(embedding)}, ${content});
  `);
}

export async function searchSimilarKnowledge(
  queryEmbedding: number[],
  limit: number = 5
) {
  const results = await db.execute(sql`
    SELECT 
      id,
      content,
      vector_distance(embedding, ${JSON.stringify(queryEmbedding)}) as distance
    FROM knowledge_vector_index
    ORDER BY distance ASC
    LIMIT ${limit};
  `);

  return results;
}
```

#### 5.4.2 ハイブリッド検索（Vector + FTS）

```typescript
export async function hybridSearch(
  query: string,
  queryEmbedding: number[],
  limit: number = 5
) {
  // 1. ベクトル検索
  const vectorResults = await searchSimilarKnowledge(queryEmbedding, limit * 2);
  
  // 2. 全文検索（FTS5）
  const ftsResults = await db.execute(sql`
    SELECT 
      id,
      content,
      rank
    FROM knowledge_fts
    WHERE knowledge_fts MATCH ${query}
    ORDER BY rank
    LIMIT ${limit * 2};
  `);

  // 3. 結果をマージしてスコアリング
  const merged = mergeSearchResults(vectorResults, ftsResults, query);
  
  // 4. 上位N件を返す
  return merged.slice(0, limit);
}

function mergeSearchResults(
  vectorResults: any[],
  ftsResults: any[],
  query: string
) {
  const scoreMap = new Map<number, number>();
  
  // ベクトル検索のスコア（距離が小さいほど高スコア）
  vectorResults.forEach((result, index) => {
    const score = 1 / (1 + result.distance) * (vectorResults.length - index);
    scoreMap.set(result.id, (scoreMap.get(result.id) || 0) + score);
  });
  
  // 全文検索のスコア
  ftsResults.forEach((result, index) => {
    const score = (ftsResults.length - index) * 0.5; // ベクトル検索より低めの重み
    scoreMap.set(result.id, (scoreMap.get(result.id) || 0) + score);
  });
  
  // スコアでソート
  const merged = Array.from(scoreMap.entries())
    .map(([id, score]) => ({
      id,
      score,
    }))
    .sort((a, b) => b.score - a.score);
  
  return merged;
}
```

### 5.5 Structured Outputs実装

#### 5.5.1 議事録生成での使用

```typescript
// server/actions/generate-meeting-note.ts
import { openai } from '@/lib/openai';

const meetingNoteSchema = {
  type: "object",
  properties: {
    title: {
      type: "string",
      description: "会議のタイトル",
    },
    summary: {
      type: "string",
      description: "3-5文程度の要約",
    },
    agendaItems: {
      type: "array",
      items: { type: "string" },
      description: "議題一覧",
    },
    discussionDetails: {
      type: "string",
      description: "Markdown形式の詳細な議論内容",
    },
    decisions: {
      type: "array",
      items: {
        type: "object",
        properties: {
          item: { type: "string" },
          details: { type: "string" },
        },
        required: ["item"],
      },
    },
    actionItems: {
      type: "array",
      items: {
        type: "object",
        properties: {
          title: { type: "string" },
          assignee: { type: "string" },
          dueDate: { type: "string", format: "date" },
          priority: {
            type: "string",
            enum: ["high", "medium", "low"],
          },
        },
        required: ["title"],
      },
    },
    nextMeeting: {
      type: "object",
      properties: {
        date: { type: "string", format: "date" },
        time: { type: "string" },
        agenda: { type: "string" },
      },
    },
    additionalNotes: {
      type: "string",
      description: "追加メモ・補足情報",
    },
  },
  required: ["title", "summary", "agendaItems", "discussionDetails"],
  additionalProperties: false,
};

export async function generateMeetingNote(
  transcription: string,
  userId: number,
  projectId: number
) {
  // プロジェクトのコンテキストを取得
  const project = await db.query.projects.findFirst({
    where: eq(projects.id, projectId),
  });

  const userRole = await db.query.userRoles.findFirst({
    where: and(
      eq(userRoles.userId, userId),
      eq(userRoles.projectId, projectId)
    ),
  });

  const completion = await openai.chat.completions.create({
    model: "gpt-4",
    messages: [
      {
        role: "system",
        content: `あなたは実務・プロダクト利用を前提とした、プロフェッショナルな議事録生成AIです。
情報を一切省略せず、構造化によって読みやすくすることを心がけてください。

プロジェクト: ${project?.name}
ユーザーの立場: ${userRole?.role || "参加者"}`,
      },
      {
        role: "user",
        content: `以下の文字起こしから議事録を生成してください:\n\n${transcription}`,
      },
    ],
    response_format: {
      type: "json_schema",
      json_schema: {
        name: "meeting_note",
        schema: meetingNoteSchema,
        strict: true,
      },
    },
  });

  const result = JSON.parse(completion.choices[0].message.content);
  
  // データベースに保存
  const note = await db.insert(meetingNotes).values({
    userId,
    projectId,
    title: result.title,
    summary: result.summary,
    agendaItems: JSON.stringify(result.agendaItems),
    discussionDetails: result.discussionDetails,
    decisions: JSON.stringify(result.decisions),
    actionItems: JSON.stringify(result.actionItems),
    nextMeeting: JSON.stringify(result.nextMeeting),
    additionalNotes: result.additionalNotes,
    formattedMinutes: formatMeetingNoteAsMarkdown(result),
  });

  return note;
}

function formatMeetingNoteAsMarkdown(data: any): string {
  return `# ${data.title}

## 要約
${data.summary}

## 議題
${data.agendaItems.map((item: string) => `- ${item}`).join('\n')}

## 議論内容
${data.discussionDetails}

## 決定事項
${data.decisions.map((d: any) => `- **${d.item}**: ${d.details || ''}`).join('\n')}

## アクションアイテム
${data.actionItems.map((a: any) => `- ${a.title} (${a.priority || 'medium'})`).join('\n')}

${data.nextMeeting ? `## 次回会議\n日時: ${data.nextMeeting.date} ${data.nextMeeting.time}\n議題: ${data.nextMeeting.agenda}` : ''}

${data.additionalNotes ? `## 追加メモ\n${data.additionalNotes}` : ''}
`;
}
```

---

## 6. データベース拡張設計

### 6.1 新規テーブル定義（Drizzle Schema）

```typescript
// db/schema-extensions.ts
import { sqliteTable, text, integer, real } from 'drizzle-orm/sqlite-core';
import { relations } from 'drizzle-orm';

// フォルダ修正履歴
export const folderCorrections = sqliteTable('folder_corrections', {
  id: integer('id').primaryKey({ autoIncrement: true }),
  userId: integer('user_id').notNull(),
  projectId: integer('project_id').notNull(),
  originalPath: text('original_path').notNull(), // JSON配列
  correctedPath: text('corrected_path').notNull(), // JSON配列
  content: text('content').notNull(),
  createdAt: integer('created_at', { mode: 'timestamp' }).notNull(),
});

// ベクトル埋め込み
export const knowledgeEmbeddings = sqliteTable('knowledge_embeddings', {
  id: integer('id').primaryKey({ autoIncrement: true }),
  knowledgeId: integer('knowledge_id').notNull(), // meetingNotes.id or files.id
  embedding: text('embedding').notNull(), // JSON配列（1536次元）
  content: text('content').notNull(),
  createdAt: integer('created_at', { mode: 'timestamp' }).notNull(),
});

// 音声波形データ（Peaks.js用）
export const audioWaveforms = sqliteTable('audio_waveforms', {
  id: integer('id').primaryKey({ autoIncrement: true }),
  recordingId: integer('recording_id').notNull(),
  waveformData: text('waveform_data').notNull(), // JSON配列
  peaksData: text('peaks_data'), // Peaks.js用のDATファイル（Base64）
  createdAt: integer('created_at', { mode: 'timestamp' }).notNull(),
});

// AI学習データ
export const aiLearningData = sqliteTable('ai_learning_data', {
  id: integer('id').primaryKey({ autoIncrement: true }),
  userId: integer('user_id').notNull(),
  dataType: text('data_type', { enum: ['folder_correction', 'tone_learning', 'content_preference'] }).notNull(),
  inputData: text('input_data').notNull(), // JSON
  outputData: text('output_data').notNull(), // JSON
  createdAt: integer('created_at', { mode: 'timestamp' }).notNull(),
});
```

### 6.2 インデックス最適化

```typescript
// db/indexes.ts
import { sql } from 'drizzle-orm';

// ベクトル検索インデックス（DiskANN）
export async function createVectorIndexes() {
  await db.execute(sql`
    CREATE VIRTUAL TABLE IF NOT EXISTS knowledge_vector_index 
    USING diskann(
      id INTEGER PRIMARY KEY,
      embedding VECTOR(1536),
      content TEXT
    );
  `);
}

// 全文検索インデックス（FTS5）
export async function createFTSIndexes() {
  await db.execute(sql`
    CREATE VIRTUAL TABLE IF NOT EXISTS knowledge_fts 
    USING fts5(
      id,
      title,
      content,
      content=meetingNotes,
      content_rowid=id
    );
  `);
}

// 通常のインデックス
export async function createStandardIndexes() {
  await db.execute(sql`
    CREATE INDEX IF NOT EXISTS idx_files_project_parent 
    ON files(projectId, parentFileId);
  `);

  await db.execute(sql`
    CREATE INDEX IF NOT EXISTS idx_meeting_notes_created 
    ON meetingNotes(createdAt DESC);
  `);

  await db.execute(sql`
    CREATE INDEX IF NOT EXISTS idx_tasks_due_date 
    ON tasks(dueDate ASC) WHERE status != 'completed';
  `);
}
```

---

## 7. API詳細設計

### 7.1 tRPC Router拡張

```typescript
// server/routers/folder.ts
import { z } from 'zod';
import { router, protectedProcedure } from '../trpc';
import { suggestFolderStructure } from '@/server/actions/ai-folder-classification';

export const folderRouter = router({
  generateStructure: protectedProcedure
    .input(
      z.object({
        projectId: z.number(),
        context: z.string(),
      })
    )
    .mutation(async ({ input, ctx }) => {
      const structure = await suggestFolderStructure(
        input.context,
        input.projectId,
        ctx.user.id
      );
      return structure;
    }),

  suggestLocation: protectedProcedure
    .input(
      z.object({
        projectId: z.number(),
        content: z.string(),
      })
    )
    .query(async ({ input, ctx }) => {
      // 類似コンテンツを検索して提案
      // ...
    }),

  move: protectedProcedure
    .input(
      z.object({
        fileId: z.number(),
        targetFolderId: z.number().nullable(),
      })
    )
    .mutation(async ({ input, ctx }) => {
      await db
        .update(files)
        .set({ parentFileId: input.targetFolderId })
        .where(eq(files.id, input.fileId));
    }),

  getTree: protectedProcedure
    .input(z.object({ projectId: z.number() }))
    .query(async ({ input, ctx }) => {
      const allFiles = await db
        .select()
        .from(files)
        .where(eq(files.projectId, input.projectId));

      // ツリー構造に変換
      return buildTree(allFiles);
    }),
});

function buildTree(files: File[]): FolderTree {
  const tree: FolderTree = {};
  
  for (const file of files) {
    if (!file.parentFileId) {
      // ルートレベル
      tree[file.id] = {
        ...file,
        children: [],
      };
    } else {
      // 子要素として追加
      // ...
    }
  }
  
  return tree;
}
```

### 7.2 バッチ処理API

```typescript
// app/api/batch/import-notion/route.ts
import { NextRequest, NextResponse } from 'next/server';
import { importNotionPages } from '@/server/actions/notion-import';

export async function POST(request: NextRequest) {
  const { userId } = await request.json();
  
  // バッチジョブを開始（非同期）
  const jobId = `import-notion-${userId}-${Date.now()}`;
  
  // バックグラウンドで実行
  importNotionPages(userId).catch(console.error);
  
  return NextResponse.json({ jobId, status: 'processing' });
}

export async function GET(request: NextRequest) {
  const { searchParams } = new URL(request.url);
  const jobId = searchParams.get('jobId');
  
  // ジョブの進捗を取得
  const progress = await getJobProgress(jobId);
  
  return NextResponse.json(progress);
}
```

---

## 8. UI/UX詳細設計

### 8.1 オンボーディングUI詳細

**コンポーネント構造**:

```
OnboardingWizard
├── PlanSelectionStep
│   ├── PlanCard (One)
│   └── PlanCard (Company)
├── BusinessInputStep
│   ├── BusinessCountInput
│   └── BusinessNameInputs (動的)
├── DepartmentInputStep
│   ├── DepartmentCountInput (事業ごと)
│   └── DepartmentNameInputs (動的)
├── PurposeSelectionStep
│   └── PurposeCheckboxes
└── PreviewStep
    ├── FolderStructurePreview
    └── EditButton
```

**アニメーション仕様**:
- ステップ遷移時: フェードアウト → スライド → フェードイン
- 入力フィールド: フォーカス時にスケールアップ（1.02倍）
- 完了時: チェックマークアニメーション

### 8.2 Graph View UI詳細

**インタラクション**:
- マウスホイール: ズームイン/アウト
- ドラッグ: パン（移動）
- ノードクリック: 詳細パネルを表示
- ノードダブルクリック: 該当ファイルを開く
- エッジホバー: 関連度を表示

**視覚的フィードバック**:
- ノードのサイズ: 関連ナレッジの数に比例
- ノードの色: プロジェクトごとに異なる色
- エッジの太さ: 関連度に比例
- アニメーション: ノードの追加時にフェードイン

---

## 9. パフォーマンス要件

### 9.1 レスポンス時間目標

| 操作 | 目標時間 | 最大許容時間 |
|------|---------|-------------|
| ページロード | < 2秒 | < 5秒 |
| 録音開始 | < 500ms | < 1秒 |
| 議事録生成 | < 30秒 | < 60秒 |
| Graph View描画 | < 1秒 | < 3秒 |
| 検索結果表示 | < 500ms | < 2秒 |

### 9.2 スケーラビリティ目標

- 同時ユーザー数: 1,000ユーザー
- 1ユーザーあたりのナレッジ数: 10,000件まで
- 1プロジェクトあたりのファイル数: 1,000件まで
- Graph Viewのノード数: 10,000ノードまでスムーズに動作

---

## 10. セキュリティ要件

### 10.1 データ暗号化

- アクセストークン: AES-256で暗号化して保存
- 音声ファイル: S3サーバー側暗号化（SSE）
- 通信: HTTPS必須（TLS 1.3）

### 10.2 認証・認可

- BetterAuthによるセッション管理
- JWT + HttpOnly Cookie
- フォルダ単位での権限チェック
- APIレート制限（100リクエスト/分）

---

## 11. テスト要件

### 11.1 単体テスト

- 各Server Actionのテスト
- ユーティリティ関数のテスト
- カバレッジ目標: 80%以上

### 11.2 統合テスト

- APIエンドポイントのテスト
- データベース操作のテスト
- 外部サービス連携のテスト

### 11.3 E2Eテスト

- Playwrightによる主要フローのテスト
- 録音 → 議事録生成 → タスク作成のフロー
- オンボーディングフロー

---

## 12. デプロイメント要件

### 12.1 環境構成

- **開発環境**: ローカル（Docker Compose）
- **ステージング環境**: Vercel Preview
- **本番環境**: Vercel Production + Turso Production

### 12.2 CI/CD

- GitHub Actionsによる自動デプロイ
- プルリクエスト時に自動テスト実行
- マージ時に自動デプロイ（ステージング）
- タグ作成時に本番デプロイ

---

**文書終了**

*本要件定義書は、Actory - 行動直結型ナレッジOSの実装フェーズにおける詳細な技術仕様を記載したものです。requirements.mdの機能要件と技術的実現可能性レポートの検証結果を統合し、実装可能な形で詳細化しています。*

引用文献
Navigating Safari/iOS PWA Limitations and Bugs: Essential Tips and Tricks - Vinova SG, 1月 1, 2026にアクセス、 https://vinova.sg/navigating-safari-ios-pwa-limitations/
Configuring your app for media playback | Apple Developer Documentation, 1月 1, 2026にアクセス、 https://developer.apple.com/documentation/avfoundation/configuring-your-app-for-media-playback
ReactJS Web App Cannot Access Microphone When Mobile Device Is Locked, 1月 1, 2026にアクセス、 https://stackoverflow.com/questions/79810064/reactjs-web-app-cannot-access-microphone-when-mobile-device-is-locked
iOS PWA Background Audio Support [closed] - Stack Overflow, 1月 1, 2026にアクセス、 https://stackoverflow.com/questions/60003027/ios-pwa-background-audio-support
Background Runner Capacitor Plugin API, 1月 1, 2026にアクセス、 https://capacitorjs.com/docs/apis/background-runner
Audio (expo-audio) - Expo Documentation, 1月 1, 2026にアクセス、 https://docs.expo.dev/versions/v53.0.0/sdk/audio/
Webpack fails to resolve React hooks from better-auth/react with Next.js 15 and React 19 #5458 - GitHub, 1月 1, 2026にアクセス、 https://github.com/better-auth/better-auth/issues/5458
Learnings from a d3.js addict on starting with canvas | Visual Cinnamon, 1月 1, 2026にアクセス、 https://www.visualcinnamon.com/2015/11/learnings-from-a-d3-js-addict-on-starting-with-canvas/
Big d3.js graph, canvas or server-side rendering? - Stack Overflow, 1月 1, 2026にアクセス、 https://stackoverflow.com/questions/17409978/big-d3-js-graph-canvas-or-server-side-rendering
A Look At Graph Visualization With Sigma React - William Lyon, 1月 1, 2026にアクセス、 https://lyonwj.com/blog/sigma-react-graph-visualization
Creating a Force Graph using React, D3 and PixiJS - DEV Community, 1月 1, 2026にアクセス、 https://dev.to/gilfink/creating-a-force-graph-using-react-d3-and-pixijs-182n
Whisper API, increase file limit >25 MB - OpenAI Developer Community, 1月 1, 2026にアクセス、 https://community.openai.com/t/whisper-api-increase-file-limit-25-mb/566754
Speech to text | OpenAI API, 1月 1, 2026にアクセス、 https://platform.openai.com/docs/guides/speech-to-text
Splitting Audio files by detecting silence using FFMPEG - EF Computer, 1月 1, 2026にアクセス、 https://efcomputer.net.au/blog/splitting-audio-files-by-detecting-silence-using-ffmpeg/
Online video editor: Why I switched from FFmpeg WASM to a server-side solution, 1月 1, 2026にアクセス、 https://news.ycombinator.com/item?id=34903898
Build error after migrating to React 19 and Next.js 15 - Better Auth - Answer Overflow, 1月 1, 2026にアクセス、 https://www.answeroverflow.com/m/1315492248703467520
Better Auth's React hooks fail when used with Next.js 15 App Router, throwing "Cannot read properties of null (reading 'useRef')" error. · Issue #3123 - GitHub, 1月 1, 2026にアクセス、 https://github.com/better-auth/better-auth/issues/3123
Fixing Next.js 14/15 and React 19/18 Compatibility Issues with next-auth and Google Provider | by Bonface Alfonce | Medium, 1月 1, 2026にアクセス、 https://medium.com/@bonfacealfonce/fixing-next-js-14-15-and-react-19-18-compatibility-issues-with-next-auth-and-google-provider-d0258542bbcb
Next.js 15, 1月 1, 2026にアクセス、 https://nextjs.org/blog/next-15
Next.js 15.1, 1月 1, 2026にアクセス、 https://nextjs.org/blog/next-15-1
SVG Vs. Canvas: A Comparison - Medium, 1月 1, 2026にアクセス、 https://medium.com/stackanatomy/svg-vs-canvas-a-comparison-1b58e6c84326
SVG versus Canvas: Which technology to choose and why? - JointJS, 1月 1, 2026にアクセス、 https://www.jointjs.com/blog/svg-versus-canvas
HTML5 SVG vs Canvas for big number of lines? - Stack Overflow, 1月 1, 2026にアクセス、 https://stackoverflow.com/questions/13962406/html5-svg-vs-canvas-for-big-number-of-lines
Understanding the Graph View core - Developers: Plugin & API - Obsidian Forum, 1月 1, 2026にアクセス、 https://forum.obsidian.md/t/understanding-the-graph-view-core/41020
What does Obsidian use to create their graph view? : r/ObsidianMD - Reddit, 1月 1, 2026にアクセス、 https://www.reddit.com/r/ObsidianMD/comments/1mhujgy/what_does_obsidian_use_to_create_their_graph_view/
Frameworks for working with graph visualizations, which one do you prefer? - Reddit, 1月 1, 2026にアクセス、 https://www.reddit.com/r/reactjs/comments/1f9lis9/frameworks_for_working_with_graph_visualizations/
How to make a 10,000 node graph performant : r/reactjs - Reddit, 1月 1, 2026にアクセス、 https://www.reddit.com/r/reactjs/comments/1epvcol/how_to_make_a_10000_node_graph_performant/
IOS PWA: Music playback stops when locked - Reddit, 1月 1, 2026にアクセス、 https://www.reddit.com/r/PWA/comments/1h2vmod/ios_pwa_music_playback_stops_when_locked/
Mediarecorder stucks on iOS as PWA app, how to clean all tracks? - Stack Overflow, 1月 1, 2026にアクセス、 https://stackoverflow.com/questions/79744644/mediarecorder-stucks-on-ios-as-pwa-app-how-to-clean-all-tracks
Maintaining WebSocket on screen lock : r/PWA - Reddit, 1月 1, 2026にアクセス、 https://www.reddit.com/r/PWA/comments/1psbznj/maintaining_websocket_on_screen_lock/
Media Playback with PWA: keep playing when phone locked? - Feature - Discourse Meta, 1月 1, 2026にアクセス、 https://meta.discourse.org/t/media-playback-with-pwa-keep-playing-when-phone-locked/182219?tl=en
[AskJS] Report into PWA Limitations in iOS : r/javascript - Reddit, 1月 1, 2026にアクセス、 https://www.reddit.com/r/javascript/comments/oyc9pc/askjs_report_into_pwa_limitations_in_ios/
Record and Play Sound in Ionic 8, Angular 20, and Capacitor Mobile App - Djamware.com, 1月 1, 2026にアクセス、 https://www.djamware.com/post/5a18411b80aca75eadc12d6d/record-and-play-sound-in-ionic-8-angular-20-and-capacitor-mobile-app
Capacitor plugin to record audio on iOS and Android, keep active in background - GitHub, 1月 1, 2026にアクセス、 https://github.com/Cap-go/capacitor-audio-recorder/
hyochan/react-native-audio-recorder-player - GitHub, 1月 1, 2026にアクセス、 https://github.com/hyochan/react-native-audio-recorder-player
What is the difference between ffmpeg, ffmpeg wasm, and ffmpeg in native app? - Stack Overflow, 1月 1, 2026にアクセス、 https://stackoverflow.com/questions/70707312/what-is-the-difference-between-ffmpeg-ffmpeg-wasm-and-ffmpeg-in-native-app
ffmpeg : split a video using silencedetect? - Super User, 1月 1, 2026にアクセス、 https://superuser.com/questions/931533/ffmpeg-split-a-video-using-silencedetect
Wavesurfer causing page renderer to crash (too many DOM nodes) · Issue #3696 - GitHub, 1月 1, 2026にアクセス、 https://github.com/katspaugh/wavesurfer.js/issues/3696
Audio files are too large, loading time is too slow, how to optimize? · Issue #1563 · katspaugh/wavesurfer.js - GitHub, 1月 1, 2026にアクセス、 https://github.com/katspaugh/wavesurfer.js/issues/1563
Peaks.js: Navigating Audio Waveforms Like a Pro, 1月 1, 2026にアクセス、 https://zolmok.org/peaks-js/
SQLite Retrieval Augmented Generation and Vector Search - Turso, 1月 1, 2026にアクセス、 https://turso.tech/blog/sqlite-retrieval-augmented-generation-and-vector-search
You Don't Need A Separate Vector Database - Turso, 1月 1, 2026にアクセス、 https://turso.tech/blog/you-dont-need-a-separate-vector-database
The space complexity of vector indexes in LibSQL - Turso, 1月 1, 2026にアクセス、 https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql
Approximate nearest neighbor search with DiskANN in libSQL - Turso, 1月 1, 2026にアクセス、 https://turso.tech/blog/approximate-nearest-neighbor-search-with-diskann-in-libsql
Indexing sparse vectors with Turso, 1月 1, 2026にアクセス、 https://turso.tech/blog/indexing-sparse-vectors-with-turso
[OLD] Drizzle Relations, 1月 1, 2026にアクセス、 https://orm.drizzle.team/docs/relations
Drizzle Relations, 1月 1, 2026にアクセス、 https://orm.drizzle.team/docs/relations-v2
Everything is Context: Agentic File System Abstraction for Context Engineering - arXiv, 1月 1, 2026にアクセス、 https://arxiv.org/html/2512.05470v1
[2512.05470] Everything is Context: Agentic File System Abstraction for Context Engineering - arXiv, 1月 1, 2026にアクセス、 https://www.arxiv.org/abs/2512.05470
Structured model outputs | OpenAI API, 1月 1, 2026にアクセス、 https://platform.openai.com/docs/guides/structured-outputs
Introducing Structured Outputs in the API - OpenAI, 1月 1, 2026にアクセス、 https://openai.com/index/introducing-structured-outputs-in-the-api/